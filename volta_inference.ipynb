{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VOLTA inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code intents to demonstrate simple steps to perform inference on a pretrained VOLTA model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to load the weights into the memory as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "checkpoint_path = 'pretrained_weights/ovarian/checkpoint.pth.tar'\n",
    "checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "state_dict = checkpoint['state_dict']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to construct the model, set the loaded weights into it, and set it to the evaluation mode. Please note that the below configuration for the model remain the same for all of our pretrained models across all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model message:  <All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "import backbones\n",
    "import moco.mocov3.builder\n",
    "\n",
    "model = moco.mocov3.builder.MoCoV3(\n",
    "    base_encoder=backbones.__dict__[\"preact_resnet18\"], \n",
    "    dim=64, \n",
    "    m=0.999, \n",
    "    mlp=[128], \n",
    "    prediction_head=32, \n",
    "    mlp_embedding=False, \n",
    "    spectral_normalization=False, \n",
    "    queue_size=65536, \n",
    "    teacher=True)\n",
    "\n",
    "print('loading model message: ', model.load_state_dict(state_dict, strict=True))\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to prepare the transformation pipeline. This pipeline includes 3 steps:\n",
    "1. Resizing the cells into the models input size ($32\\times32$ for all of our models)\n",
    "2. Normalize the cell with the normalization vector of the dataset used for the training of the model (in this case ovarian dataset)\n",
    "3. Convert the cell vector to a Pytorch tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations\n",
    "import albumentations.pytorch\n",
    "import cv2\n",
    "from dataset.ovarian.transform import get_cell_normalization # Note: use the same normalization as the training data of the pretrained model\n",
    "\n",
    "transforms = albumentations.Compose([\n",
    "    albumentations.Resize(height=32, width=32, interpolation=cv2.INTER_CUBIC), # step 1: resize\n",
    "    get_cell_normalization(), # step 2: normalization\n",
    "    albumentations.pytorch.ToTensorV2(transpose_mask=True), # step 3: to tensor\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "source_image_path = 'examples/example1.png'\n",
    "\n",
    "# read the image with PIL and convert to numpy array\n",
    "img = np.array(Image.open(source_image_path))\n",
    "\n",
    "# resize, normalize, and convert to tensor\n",
    "img = transforms(image=img)['image']\n",
    "\n",
    "# pass the image through the model\n",
    "embedding = model(img.unsqueeze(0)) # Note: unsqueeze to add a batch dimension\n",
    "\n",
    "# print the shape of the embedding\n",
    "print('final embedding shape: ', embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 3 images: ['examples/example3.png', 'examples/example2.png', 'examples/example1.png']\n",
      "final embedding shape:  torch.Size([3, 512])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# get the list of all images in the examples folder\n",
    "source_image_path = [os.path.join('examples', f) for f in os.listdir('examples') if f.endswith('.png')]\n",
    "\n",
    "print(f'found {len(source_image_path)} images: {source_image_path}')\n",
    "\n",
    "# read the image with PIL and convert to numpy array\n",
    "img = [np.array(Image.open(f)) for f in source_image_path]\n",
    "\n",
    "# resize, normalize, convert to tensor, and stack\n",
    "img = torch.stack([transforms(image=i)['image'] for i in img])\n",
    "\n",
    "# pass the image through the model\n",
    "embedding = model(img)\n",
    "\n",
    "# print the shape of the embedding\n",
    "print('final embedding shape: ', embedding.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
